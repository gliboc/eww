Question 1

- [x] a) Initialize an agent using `agent:init`, or simply by spawning its core function, `agent:loop`. This loop calls differents handlers to treat the messages it receives. These handlers are defined in `handlers.erl` and `transfer.erl`.
    - [x] test `init` agent
    - [x] test `loop` function
        - [x] test `loop` handlers
            - [ ] write extensive testing for `handlers:handle_cmd`
            - [ ] `handlers:handle_msg` 
            - [ ]  `transfer:handle_data` 
- [x] b) Spawning a ring-shaped topology.
  - [x] test 
    - [x] `ring_topology` 
    - [x] `last_init` 
- [x] c) `agent:join` allows an agent to join the topology. `new_node` spawns an agent and has it join the topology.
  - [x] test
    - [x] `join` 
    - [x] `new_node` 
- [x] d) `agent:kill` asks the platform to kill a designated target. `destroy` makes the network self-destruct.
  - [x] test `kill` - works for any target node
  - [x] `destroy` works 
- [ ] e) / Use 'Naming processes' section of LYEFGG



Question 2

- [x] a) `com:broadcast` makes sure a Msg is delivered to every node in the system. This might be redundant with certain types of messages that are automatically transmitted, but it doesn't cause any errors. This is great to be used to start an election for example, by sending the token `start_election` to everyone.
    - [x] test
- [ ] b)
- [x] c) Lelann's lalgorithm. Using the `{elect, Proc}` token that is being passed around. Bugfix : the case when p == q is probably dropped because of Ref checking. Will go over that part very soon. Also, now procs have a name `proc` that is an UUID generated by uuid4 (fully random version). These UUIDs can be compared two-by-two. `process_msg` takes the major blow in treating the election process. An election can be started only by sending a token `{start_election}` to a node. `agent:start_elect` can do this. 
  It is possible to check the election state of the network with the function `give_status`.
  - [x] test `give_status`
  - [x] test `start_elect`-> to have several nodes engage as candidate, send them each a start_elect token. For example, `broadcast` all the nodes a `start_election` token.



Question 3

- [x] a) `handle_data` is able to receive a signed or unsigned piece of data. If it's unsigned, there should be a Pid in the message to which it sends an UUID key for retrieving the data. Else, it simply stores both the data and the key. Also, it stores its content in a `data/` dir.
    - [x] tested with a video file
- [x] b) done using `retrieve_data` which is called when a node receives a request with a key that's in its keys list. If it hasn't got it, it sends the request to its next peer. If the key is never found, `fail_msg` warns the client. Todo : recycle `fail_msg`
  - [x] test
- [x] c) The terminate function transfers all of a node's data and keys to its peer. It sends pre-signed data to its peer, so the peer doesn't have to generate an UUID; it just writes the data and stores the key as its own.
  - [x] test function : `transfer_tests:terminate_test`
- [x] d) A message, `{del, Key}`, can be passed around to delete the associated file from all nodes of the network.
  - [x] test
- [ ] e)
- [ ] f)
- [ ] g)



Question 4 CLI `client.erl`

- [x] a) Function `client:start`. Very basic atm
    - [ ] test
- [x] b) Function `client:retire`. Sketchy at best.
  - [ ] test
- [x] c) `client:push` function. Very bad atm : it just sends data to the first node it knows about.
  - [ ] test
- [x] d) This is the `client:pull` function. It stores its data in the `data_rcv` directory,.
  - [x] test
- [x] e) Function `client:release`, using a msg structure caught by `handlers:handle_msg` , treated by `handlers:process_msg`, executed by `transfer:delete_data`. This is actually the same as q3 d)
  - [x] test `release_test` function, using `?assert` macro 
- [x] f) Using `client:ping` : the ping goes through all the network, adding up the number of nodes, the number of hashes kept (ie the number of messages that passed through the network), and the number of keys (ie the number of files that are being kept).
  - [x] test - works fine
  - [ ] Todo: find a way to give out the total size of the files, to give node-specific information (by naming them ?), and to find the size of the biggest/lowest file that is being stored. Other cool data that would require nodes to maintain an history : size of data that was exchanged since the network started, number of files since the network started, number of push/pull operations, up-time of each node, ping time of each node in ms, ping time of the global network.
- [x] g) `client:deploy` deploy a ring-shaped topology of nodes using the `Nodes` list of adresses.
  - [ ] test




Others:

- [ ] Implement authentification using indigo-dc/oidcc erlang implementation of the OpenId Connect protocol
- [ ] Have functions reply with `{ok, Result}` or `{error, Reason}` forms, and log the errors
- [x] Type analysis using Dialyzer
- [ ] Implement termination to be used in the case when it is useful (for example, wait for some large data to have been completely scattered on the ring before querying it)



Problems:

- [ ] when a node dies, the redundancy number of its data is reduced by 1, and there is no control mechanism to ensure it goes up afterwards. So, if all the original nodes die, data may disappear from the network.



What I'll probably won't do but found interesting:

- [ ] routing using the key. In algorithms such as Chord or Kademlia, the structure of the key helps in finding the node where this data is stored. This leads to faster lookup in theory.
  In practice, I'm not interested in very big scalability just yet and prefer to implement features and fault tolerance, since I plan to use this software for my own purposes.
- [ ] An elaborate topology. I found de Bruijn graphs quite interesting, as in Koorde they allow an optimal lookup for data ($$O(\log(n))$$) - the structure in itself is also interesting. But - creating and managing de Bruijn graphs is difficult and less documented than rings. And, again, I wouldn't see much difference in my case.
- [ ] Termination checks and passive/active nodes. This is useful in order to prevent some activities from being launched during a time when processes are busy doing some critical task. Before implementing this, I want to identify real errors that could be solved using this safety check.
  - Possibility : wait for some large data to have been completely scattered on the ring before querying it